{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tiny_shakespeare.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it \n"
     ]
    }
   ],
   "source": [
    "print(text[:400])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "char_to_index = {ch: i for i, ch in enumerate(chars)}\n",
    "index_to_char = {i: ch for i, ch in enumerate(chars)} \n",
    "\n",
    "encode = lambda s: [char_to_index[c] for c in s]\n",
    "decode = lambda ids: ''.join([index_to_char[i] for i in ids])\n",
    "\n",
    "input_txt = \"hello world\"\n",
    "encoded_data = encode(input_txt)\n",
    "decoded_data = decode(encoded_data)\n",
    "print(encoded_data)\n",
    "print(decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:400])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.9\n",
    "n = int(train_split_percentage * len(data))\n",
    "train_data = data[:n]\n",
    "validation_data = data[n:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "First Cit\n"
     ]
    }
   ],
   "source": [
    "context_length = 8\n",
    "\n",
    "print(train_data[:context_length + 1])\n",
    "print(decode(train_data[:context_length + 1].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47])\n",
      "tensor([47, 56, 57, 58,  1, 15, 47, 58])\n",
      "\n",
      "input: tensor([18]), target: 47\n",
      "input: tensor([18]), target: 47\n",
      "input: tensor([18, 47]), target: 56\n",
      "input: tensor([18, 47]), target: 56\n",
      "input: tensor([18, 47, 56]), target: 57\n",
      "input: tensor([18, 47, 56]), target: 57\n",
      "input: tensor([18, 47, 56, 57]), target: 58\n",
      "input: tensor([18, 47, 56, 57]), target: 58\n",
      "input: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "input: tensor([18, 47, 56, 57, 58]), target: 1\n",
      "input: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "input: tensor([18, 47, 56, 57, 58,  1]), target: 15\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n",
      "input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length + 1]\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print()\n",
    "\n",
    "for t in range(context_length):\n",
    "    context = x[:t + 1] \n",
    "    target = y[t]\n",
    "    print(f\"input: {context}, target: {target}\")\n",
    "    print(f\"input: {context}, target: {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "context_length = 256\n",
    "train_iters = 10000\n",
    "eval_iters = 30\n",
    "learning_rate = 6e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([64, 256])\n",
      "tensor([[54, 43, 63,  ..., 54, 43, 63],\n",
      "        [57,  1, 61,  ..., 47, 52,  1],\n",
      "        [57,  1, 58,  ..., 46, 39, 58],\n",
      "        ...,\n",
      "        [23, 17,  1,  ..., 24, 17, 10],\n",
      "        [43, 50, 54,  ...,  1, 21, 51],\n",
      "        [63, 53, 59,  ..., 63,  1, 58]])\n",
      "targets\n",
      "torch.Size([64, 256])\n",
      "tensor([[43, 63,  1,  ..., 43, 63, 12],\n",
      "        [ 1, 61, 53,  ..., 52,  1, 61],\n",
      "        [ 1, 58, 46,  ..., 39, 58, 46],\n",
      "        ...,\n",
      "        [17,  1, 27,  ..., 17, 10,  0],\n",
      "        [50, 54,  1,  ..., 21, 51, 54],\n",
      "        [53, 59,  1,  ...,  1, 58, 39]])\n",
      "----------\n",
      "input: tensor([54]), target: 43\n",
      "input: tensor([54, 43]), target: 63\n",
      "input: tensor([54, 43, 63]), target: 1\n",
      "input: tensor([54, 43, 63,  1]), target: 58\n",
      "input: tensor([54, 43, 63,  1, 58]), target: 46\n",
      "input: tensor([54, 43, 63,  1, 58, 46]), target: 43\n",
      "input: tensor([54, 43, 63,  1, 58, 46, 43]), target: 0\n",
      "input: tensor([54, 43, 63,  1, 58, 46, 43,  0]), target: 19\n",
      "input: tensor([54, 43, 63,  1, 58, 46, 43,  0, 19]), target: 56\n",
      "input: tensor([54, 43, 63,  1, 58, 46, 43,  0, 19, 56]), target: 43\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else validation_data\n",
    "    ix = torch.randint(low=0, high=len(data) - context_length, size=(batch_size, ))\n",
    "    x = torch.stack([data[i: i + context_length] for i in ix])\n",
    "    y = torch.stack([data[i + 1: i + context_length + 1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "print(\"inputs\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"targets\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "# Demo with showing only first batch and 20 char context.\n",
    "for b in range(batch_size):\n",
    "    for t in range(context_length):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"input: {context}, target: {target}\")\n",
    "\n",
    "        if t > 8:\n",
    "            break\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, context_length: int, head_dim: int = 32, causal: bool = True, dropout: float = 0.2):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.causal = causal\n",
    "        self.dropout = dropout\n",
    "        self.to_key = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.to_query = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.to_value = nn.Linear(embed_dim, head_dim, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((context_length, context_length))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, t, c = x.shape\n",
    "        k = self.to_key(x)\n",
    "        q = self.to_key(x)\n",
    "        v = self.to_key(x)\n",
    "\n",
    "        # Attention scores and masking for autoregressive for transformer decoder only LM.\n",
    "        attn_weights = q @ k.transpose(-1, -2) * (self.head_dim ** -0.5)\n",
    "        if self.causal:\n",
    "            attn_weights = attn_weights.masked_fill(self.tril[:t, :t] == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        return out, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, embed_dim: int, context_length: int, num_heads: int, head_dim: int = 32, causal: bool = True, dropout: float = 0.2\n",
    "    ):\n",
    "        \"\"\"Scripted dot product attention which splits embed dim to number of head for parallel computation in multi head attention.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.sdp_heads = nn.ModuleList([\n",
    "            ScaledDotProductAttention(\n",
    "                embed_dim=embed_dim, context_length=context_length, head_dim=embed_dim // num_heads, causal=causal, dropout=dropout\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_attn_out = []\n",
    "        head_attn_weights_out = []\n",
    "        for h in self.sdp_heads:\n",
    "            attn_out, attn_weights = h(x)\n",
    "            head_attn_out.append(attn_out)\n",
    "            head_attn_weights_out.append(attn_weights)\n",
    "\n",
    "        head_attn_out = torch.cat(head_attn_out, dim=-1)\n",
    "        head_attn_weights_out = torch.cat(head_attn_weights_out, dim=-1)\n",
    "        out = self.projection(head_attn_out)\n",
    "        out = F.dropout(out, p=self.dropout)\n",
    "        return out, head_attn_weights_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, embed_dim: int, context_length: int, num_heads: int, head_dim: int = 32, causal: bool = True, dropout: float = 0.2\n",
    "    ):\n",
    "        \"\"\"Using prenorm like gpt. Since multi head attention implementation return attention output and weights as tuple\n",
    "        the 0th index is used mha for outputs. \n",
    "        \"\"\"\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mhsa = torch.nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            torch.jit.script(\n",
    "                MultiHeadAttention(\n",
    "                    embed_dim=embed_dim, context_length=context_length, num_heads=num_heads, head_dim=head_dim, causal=causal, dropout=dropout\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.mhsa = torch.jit.script(self.mhsa)\n",
    "        print(self.mhsa.code)\n",
    "\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim),\n",
    "            nn.Dropout(p=dropout),\n",
    "        )        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mhsa(x)[0]\n",
    "        x = x + self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of direct logits embedding is generated from input indices which is reshaped to vocab size for softmax in loss. Also positional embedding is added to each index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size: int,\n",
    "            device: str,\n",
    "            context_length: int = 8,\n",
    "            embed_dim: int = 32,\n",
    "            head_dim: int = 32,\n",
    "            num_heads: int = 4,\n",
    "            dropout: float = 0.2,\n",
    "    ):\n",
    "        super(GPTLanguageModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.context_length = context_length\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=context_length, embedding_dim=embed_dim)\n",
    "        \n",
    "        # self.attn_head = ScaledDotProductAttention(embed_dim=embed_dim, head_dim=head_dim, context_length=context_length)\n",
    "        # self.lm_head = nn.Linear(head_dim, vocab_size)\n",
    "\n",
    "        # self.attn_head = torch.jit.script(\n",
    "        #     MultiHeadAttention(\n",
    "        #         embed_dim=embed_dim, context_length=context_length, causal=True, num_heads=num_heads,\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # print(self.attn_head.code)\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            TransformerBlock(context_length=context_length, head_dim=head_dim, embed_dim=embed_dim, num_heads=num_heads, causal=True, dropout=dropout),\n",
    "            TransformerBlock(context_length=context_length, head_dim=head_dim, embed_dim=embed_dim, num_heads=num_heads, causal=True, dropout=dropout),\n",
    "            TransformerBlock(context_length=context_length, head_dim=head_dim, embed_dim=embed_dim, num_heads=num_heads, causal=True, dropout=dropout),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Logits in shape of (batch, time_dim, channel_dim) which is reshaped to 2d tensor for cross entropy loss. \n",
    "        `t` is the time dimension context_length and `c` is channel dim each token embedding.\n",
    "        \"\"\"\n",
    "        b, t = idx.shape\n",
    "\n",
    "        token_embeddings = self.token_embedding_table(idx)  # (b, t, embed_dim)\n",
    "        position_embeddings = self.position_embedding_table(torch.arange(t, device=self.device))    # (t, embed_dim)\n",
    "        x = token_embeddings + position_embeddings  # (b, t, embed_dim)\n",
    "\n",
    "        # x, attn_weights = self.attn_head(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "\n",
    "        logits = self.lm_head(x)    # (b, t, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b * t, c)\n",
    "            targets = targets.view(b * t)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.jit.export\n",
    "    def generate(self, idx, max_new_tokens: int):\n",
    "        \"\"\"Generates new token from previous token and sample top 1 from softmax probs for next id. Due to positional \n",
    "        embedding table set to context length anything beyond will cause error so idx are truncated to last context\n",
    "        indices.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_truncated = idx if idx.size(1) <= self.context_length else idx[:, -self.context_length:]\n",
    "            logits, _ = self(idx_truncated)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            ids_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, ids_next], dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = getattr(self, \"0\")\n",
      "  _1 = getattr(self, \"1\")\n",
      "  input0 = (_0).forward(input, )\n",
      "  return (_1).forward(input0, )\n",
      "\n",
      "def forward(self,\n",
      "    input: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = getattr(self, \"0\")\n",
      "  _1 = getattr(self, \"1\")\n",
      "  input0 = (_0).forward(input, )\n",
      "  return (_1).forward(input0, )\n",
      "\n",
      "def forward(self,\n",
      "    input: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  _0 = getattr(self, \"0\")\n",
      "  _1 = getattr(self, \"1\")\n",
      "  input0 = (_0).forward(input, )\n",
      "  return (_1).forward(input0, )\n",
      "\n",
      "torch.Size([16384, 65])\n",
      "tensor(4.3623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 0, 11, 29, 10, 14, 50, 37, 64, 62,  1, 37,  4, 37, 34, 49, 42, 25, 25,\n",
      "         11,  7, 33, 13, 18, 44,  3, 56, 52, 40,  8, 30, 30, 55, 30, 37,  7, 20,\n",
      "         47, 18, 26,  2, 33, 29,  7, 36, 56, 61, 56, 56, 62, 14,  5, 15,  5, 16,\n",
      "         26, 61, 33, 23, 21, 57, 18, 24, 22, 43, 45, 44, 35, 11, 42, 56, 61, 14,\n",
      "         29, 44,  7, 49, 59, 12, 30, 21, 19,  5, 26,  4, 58, 18, 41, 56,  2, 13,\n",
      "         21, 55, 49, 35,  8, 20,  4, 20, 48, 57, 37]], device='cuda:0')\n",
      "torch.Size([1, 101])\n",
      "\n",
      ";Q:BlYzx Y&YVkdMM;-UAFf$rnb.RRqRY-HiFN!UQ-XrwrrxB'C'DNwUKIsFLJegfW;drwBQf-ku?RIG'N&tFcr!AIqkW.H&HjsY\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size, device=device, context_length=context_length, embed_dim=256, num_heads=4, dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "# model = torch.jit.trace(model, (xb.to(device), yb.to(device)))\n",
    "# model = torch.jit.script(model)\n",
    "\n",
    "logits, loss = model(xb.to(device), yb.to(device))\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "pred_token_idx = model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=100)\n",
    "print(pred_token_idx)\n",
    "print(pred_token_idx.shape)\n",
    "print(decode(pred_token_idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=torch.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Batch Loss Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            with ctx:\n",
    "                logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()            \n",
    "    return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\computer\\miniconda3\\envs\\torch4\\lib\\site-packages\\torch\\autograd\\__init__.py:200: UserWarning: operator () profile_node %493 : int[] = prim::profile_ivalue(%dims.16)\n",
      " does not have profile information (Triggered internally at ..\\third_party\\nvfuser\\csrc\\graph_fuser.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0: train loss: 4.3476, val loss: 3.9937\n",
      "step: 30: train loss: 2.6456, val loss: 2.6201\n",
      "step: 60: train loss: 2.5480, val loss: 2.5463\n",
      "step: 90: train loss: 2.5338, val loss: 2.5047\n",
      "step: 120: train loss: 2.5105, val loss: 2.5265\n",
      "step: 150: train loss: 2.4912, val loss: 2.5133\n",
      "step: 180: train loss: 2.4843, val loss: 2.4958\n",
      "step: 210: train loss: 2.4869, val loss: 2.5044\n",
      "step: 240: train loss: 2.4718, val loss: 2.5022\n",
      "step: 270: train loss: 2.4556, val loss: 2.4960\n",
      "step: 300: train loss: 2.4536, val loss: 2.4874\n",
      "step: 330: train loss: 2.4661, val loss: 2.4851\n",
      "step: 360: train loss: 2.4591, val loss: 2.5088\n",
      "step: 390: train loss: 2.4555, val loss: 2.4527\n",
      "step: 420: train loss: 2.4594, val loss: 2.4853\n",
      "step: 450: train loss: 2.4335, val loss: 2.4724\n",
      "step: 480: train loss: 2.4613, val loss: 2.4853\n",
      "step: 510: train loss: 2.4479, val loss: 2.4818\n",
      "step: 540: train loss: 2.4321, val loss: 2.4613\n",
      "step: 570: train loss: 2.4315, val loss: 2.4366\n",
      "step: 600: train loss: 2.4329, val loss: 2.4568\n",
      "step: 630: train loss: 2.4154, val loss: 2.4467\n",
      "step: 660: train loss: 2.4209, val loss: 2.4276\n",
      "step: 690: train loss: 2.3674, val loss: 2.4341\n",
      "step: 720: train loss: 2.3971, val loss: 2.4115\n",
      "step: 750: train loss: 2.3796, val loss: 2.3941\n",
      "step: 780: train loss: 2.3907, val loss: 2.4377\n",
      "step: 810: train loss: 2.3244, val loss: 2.3984\n",
      "step: 840: train loss: 2.3493, val loss: 2.3690\n",
      "step: 870: train loss: 2.3325, val loss: 2.3778\n",
      "step: 900: train loss: 2.2917, val loss: 2.3371\n",
      "step: 930: train loss: 2.2757, val loss: 2.3621\n",
      "step: 960: train loss: 2.2786, val loss: 2.3217\n",
      "step: 990: train loss: 2.2673, val loss: 2.3246\n",
      "step: 1020: train loss: 2.2654, val loss: 2.3224\n",
      "step: 1050: train loss: 2.2238, val loss: 2.2714\n",
      "step: 1080: train loss: 2.2167, val loss: 2.2783\n",
      "step: 1110: train loss: 2.2072, val loss: 2.2831\n",
      "step: 1140: train loss: 2.1954, val loss: 2.2665\n",
      "step: 1170: train loss: 2.1466, val loss: 2.2081\n",
      "step: 1200: train loss: 2.1500, val loss: 2.2212\n",
      "step: 1230: train loss: 2.1351, val loss: 2.1916\n",
      "step: 1260: train loss: 2.1484, val loss: 2.2087\n",
      "step: 1290: train loss: 2.0592, val loss: 2.1660\n",
      "step: 1320: train loss: 2.0818, val loss: 2.1574\n",
      "step: 1350: train loss: 2.0625, val loss: 2.1336\n",
      "step: 1380: train loss: 2.0450, val loss: 2.1101\n",
      "step: 1410: train loss: 2.0125, val loss: 2.1138\n",
      "step: 1440: train loss: 1.9858, val loss: 2.0908\n",
      "step: 1470: train loss: 1.9759, val loss: 2.0859\n",
      "step: 1500: train loss: 1.9834, val loss: 2.0717\n",
      "step: 1530: train loss: 1.9436, val loss: 2.0574\n",
      "step: 1560: train loss: 1.9353, val loss: 2.0356\n",
      "step: 1590: train loss: 1.9282, val loss: 2.0530\n",
      "step: 1620: train loss: 1.8880, val loss: 2.0102\n",
      "step: 1650: train loss: 1.8987, val loss: 2.0179\n",
      "step: 1680: train loss: 1.8728, val loss: 1.9900\n",
      "step: 1710: train loss: 1.8486, val loss: 2.0215\n",
      "step: 1740: train loss: 1.8678, val loss: 1.9885\n",
      "step: 1770: train loss: 1.8526, val loss: 1.9957\n",
      "step: 1800: train loss: 1.8441, val loss: 1.9774\n",
      "step: 1830: train loss: 1.8309, val loss: 1.9902\n",
      "step: 1860: train loss: 1.8411, val loss: 1.9378\n",
      "step: 1890: train loss: 1.8000, val loss: 1.9623\n",
      "step: 1920: train loss: 1.8044, val loss: 1.9714\n",
      "step: 1950: train loss: 1.8011, val loss: 1.9381\n",
      "step: 1980: train loss: 1.7961, val loss: 1.9136\n",
      "step: 2010: train loss: 1.7734, val loss: 1.8851\n",
      "step: 2040: train loss: 1.7721, val loss: 1.8968\n",
      "step: 2070: train loss: 1.7841, val loss: 1.9057\n",
      "step: 2100: train loss: 1.7402, val loss: 1.8986\n",
      "step: 2130: train loss: 1.7882, val loss: 1.9016\n",
      "step: 2160: train loss: 1.7691, val loss: 1.9097\n",
      "step: 2190: train loss: 1.7349, val loss: 1.8807\n",
      "step: 2220: train loss: 1.7440, val loss: 1.8841\n",
      "step: 2250: train loss: 1.7172, val loss: 1.8591\n",
      "step: 2280: train loss: 1.6942, val loss: 1.8829\n",
      "step: 2310: train loss: 1.7292, val loss: 1.8981\n",
      "step: 2340: train loss: 1.7256, val loss: 1.9022\n",
      "step: 2370: train loss: 1.7220, val loss: 1.8813\n",
      "step: 2400: train loss: 1.7269, val loss: 1.8227\n",
      "step: 2430: train loss: 1.7006, val loss: 1.8801\n",
      "step: 2460: train loss: 1.7143, val loss: 1.8283\n",
      "step: 2490: train loss: 1.7044, val loss: 1.8491\n",
      "step: 2520: train loss: 1.6371, val loss: 1.8364\n",
      "step: 2550: train loss: 1.6844, val loss: 1.8308\n",
      "step: 2580: train loss: 1.6899, val loss: 1.8632\n",
      "step: 2610: train loss: 1.6719, val loss: 1.8308\n",
      "step: 2640: train loss: 1.6611, val loss: 1.8341\n",
      "step: 2670: train loss: 1.6594, val loss: 1.8249\n",
      "step: 2700: train loss: 1.6566, val loss: 1.8279\n",
      "step: 2730: train loss: 1.6466, val loss: 1.8248\n",
      "step: 2760: train loss: 1.6394, val loss: 1.8265\n",
      "step: 2790: train loss: 1.6349, val loss: 1.8345\n",
      "step: 2820: train loss: 1.6503, val loss: 1.8466\n",
      "step: 2850: train loss: 1.6133, val loss: 1.7792\n",
      "step: 2880: train loss: 1.6654, val loss: 1.8259\n",
      "step: 2910: train loss: 1.6050, val loss: 1.7992\n",
      "step: 2940: train loss: 1.6298, val loss: 1.7746\n",
      "step: 2970: train loss: 1.6485, val loss: 1.7957\n",
      "step: 3000: train loss: 1.6474, val loss: 1.7567\n",
      "step: 3030: train loss: 1.6343, val loss: 1.7835\n",
      "step: 3060: train loss: 1.6201, val loss: 1.7780\n",
      "step: 3090: train loss: 1.6107, val loss: 1.7986\n",
      "step: 3120: train loss: 1.5980, val loss: 1.7797\n",
      "step: 3150: train loss: 1.6259, val loss: 1.7571\n",
      "step: 3180: train loss: 1.6386, val loss: 1.7859\n",
      "step: 3210: train loss: 1.5967, val loss: 1.7895\n",
      "step: 3240: train loss: 1.5735, val loss: 1.7575\n",
      "step: 3270: train loss: 1.6127, val loss: 1.7719\n",
      "step: 3300: train loss: 1.6017, val loss: 1.7534\n",
      "step: 3330: train loss: 1.6009, val loss: 1.7569\n",
      "step: 3360: train loss: 1.5880, val loss: 1.7611\n",
      "step: 3390: train loss: 1.5855, val loss: 1.8004\n",
      "step: 3420: train loss: 1.5899, val loss: 1.7642\n",
      "step: 3450: train loss: 1.6083, val loss: 1.7754\n",
      "step: 3480: train loss: 1.6017, val loss: 1.7629\n",
      "step: 3510: train loss: 1.5747, val loss: 1.7636\n",
      "step: 3540: train loss: 1.5879, val loss: 1.7413\n",
      "step: 3570: train loss: 1.5923, val loss: 1.7643\n",
      "step: 3600: train loss: 1.5895, val loss: 1.7663\n",
      "step: 3630: train loss: 1.5693, val loss: 1.7143\n",
      "step: 3660: train loss: 1.5554, val loss: 1.7274\n",
      "step: 3690: train loss: 1.5522, val loss: 1.7695\n",
      "step: 3720: train loss: 1.5432, val loss: 1.7362\n",
      "step: 3750: train loss: 1.5932, val loss: 1.7071\n",
      "step: 3780: train loss: 1.5914, val loss: 1.7383\n",
      "step: 3810: train loss: 1.5640, val loss: 1.6965\n",
      "step: 3840: train loss: 1.5829, val loss: 1.7206\n",
      "step: 3870: train loss: 1.5816, val loss: 1.7592\n",
      "step: 3900: train loss: 1.5616, val loss: 1.7098\n",
      "step: 3930: train loss: 1.5658, val loss: 1.7115\n",
      "step: 3960: train loss: 1.5711, val loss: 1.7130\n",
      "step: 3990: train loss: 1.5598, val loss: 1.7394\n",
      "step: 4020: train loss: 1.5520, val loss: 1.7485\n",
      "step: 4050: train loss: 1.5683, val loss: 1.7748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xb, yb)\n\u001b[0;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      8\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m eval_iters \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     11\u001b[0m     \u001b[39m# losses = estimate_loss()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39m# print(f\"step: {i}: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\computer\\miniconda3\\envs\\torch4\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\computer\\miniconda3\\envs\\torch4\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(train_iters):\n",
    "    xb, yb = get_batch('train')\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % eval_iters == 0:\n",
    "        # losses = estimate_loss()\n",
    "        # print(f\"step: {i}: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "\n",
    "        x, y = get_batch('val')\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with ctx:\n",
    "            val_logits, val_loss = model(x, y)\n",
    "\n",
    "        print(f\"step: {i}: train loss: {loss:.4f}, val loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "print(f\"final train loss: {loss.item():.4f}, val loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1001])\n",
      "\n",
      "You resof thou me's of his claimen; to, hon net cousing,\n",
      "Their hearlen, and, my knows bunrooples, by merry;\n",
      "We use mittle his deadeth brother this fleshing of out,\n",
      "And next my that hon.\n",
      "Catesby to hence. No, here that me? Bushop'st pounn inded, and\n",
      "To fearsh'd next seedition, of fight\n",
      "The wings authros? Come, the Luck, but ouf the srage.\n",
      "\n",
      "DUCHESS II:\n",
      "Smorrow the us for 't was you, folly you art\n",
      "COMINUS:\n",
      "Nor the shaldieven the treape happy of loved, and all.\n",
      "They fait;\n",
      "The see forswatining? O gentle becape any heavourt\n",
      "Hare bitte chonoq him lie gentlemen'd-\n",
      "uto fawers they will\n",
      "She's spon traidf and thy me? they off me?\n",
      "\n",
      "First Citizen PRossent:\n",
      "Come have mean, indumand'd my lovoy of my relive.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "\n",
      "CAPULET:\n",
      "Therreforcia, and years, doth in I'ld no\n",
      "Which it; Doth darked his be passague rooving my purpost as be have\n",
      "That the wouldin musick.\n",
      "\n",
      "\n",
      "ELBOW:\n",
      "Unjusts most brow faniful'd Romeo, my look,\n",
      "I'Trove words no peace you lord blefen me\n",
      "your deathe, he\n",
      "Bed strew world! My Butth\n"
     ]
    }
   ],
   "source": [
    "pred_token_idx = model.generate(torch.zeros((1, 1), dtype=torch.long, device=device), max_new_tokens=800)\n",
    "# print(pred_token_idx)\n",
    "print(pred_token_idx.shape)\n",
    "print(decode(pred_token_idx[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
